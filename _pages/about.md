---
permalink: /
title: "About Me 🧑‍💻"
seo_title: "Jinjie Yuan's Homepage @ Intel"
author_profile: true
redirect_from: 
    - /about/
    - /about.html
---


I am an engineer and researcher at **Intel** 🏢. I joined Intel as an intern in 2022 and became a full-time employee in 2023. My research and work primarily focus on **AI model optimization** 🤖, including model compression (pruning, sparsity, quantization), inference acceleration ⚡, and lightweight deployment. I am dedicated to making large language models (LLMs) smaller, faster, and more efficient for deployment in resource-constrained environments such as edge devices 📱.


🎓 I received my M.S. degree in Computer Science and Technology from **Guangdong University of Technology** in 2023, supervised by Prof. [Ruichu Cai](https://ruichucai.github.io). Prior to that, I obtained my B.S. degree in Software Engineering from the same university in 2020, and was recommended for direct admission to the master's program.


Current Work 🚀
======
My recent work focuses on **Hybrid Inference for Agentic AI** 🧠. I am exploring the integration of prefix caching mechanisms with CPU offloading architectures to optimize agentic workloads. These workloads often involve long instructions, few-shot examples, and iterative reasoning. By leveraging CPU resources for computation and KV cache storage, we aim to significantly enhance inference throughput and efficiency for LLM inference.


Research Interests 🚩
======
- Natural language processing (NLP) & large language models (LLM)
- Agentic AI & efficient inference/deployment 
- Model compression & optimization (pruning, sparsity, quantization)
- Low-rank & adapter methods


💬 I am always open to discussions and potential collaborations. Please feel free to reach out to me!


Selected Publications 📚
======
For a full list, please refer to my [Google Scholar](https://scholar.google.com/citations?user=SsbhgeIAAAAJ&hl=zh-CN&oi=sra).


*   **RTTC: Reward-Guided Collaborative Test-Time Compute**
    <br>J. Pablo Muñoz\*, **Jinjie Yuan**\* (Co-first Author)
    <br>_EMNLP 2025 Findings_ · [Paper](https://arxiv.org/abs/2508.10024)

*   **Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models**
    <br>J. Pablo Muñoz\*, **Jinjie Yuan**\* (Co-first Author), Nilesh Jain
    <br>_NAACL 2025 (Oral)_ · [Paper](https://arxiv.org/abs/2501.17088) · [Code](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning)

*   **Low-Rank Adapters Meet Neural Architecture Search for LLM Compression**
    <br>J. Pablo Muñoz, **Jinjie Yuan**, Nilesh Jain
    <br>_AAAI 2025 Workshop on Connecting Low-rank Representations in AI_ · [Paper](https://arxiv.org/abs/2501.16372)

*   **SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models**
    <br>J. Pablo Muñoz\*, **Jinjie Yuan**\* (Co-first Author), Nilesh Jain
    <br>_EMNLP 2024 Findings_ · [Paper](https://arxiv.org/abs/2410.03750) · [Code](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning)

*   **Shears: Unstructured Sparsity with Neural Low-rank Adapter Search**
    <br>J. Pablo Muñoz\*, **Jinjie Yuan**\* (Co-first Author), Nilesh Jain
    <br>_NAACL 2024_ · [Paper](https://arxiv.org/abs/2404.10934) · [Code](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning)

*   **LoNAS: Elastic Low-rank Adapters for Efficient Large Language Models**
    <br>J. Pablo Muñoz\*, **Jinjie Yuan**\* (Co-first Author), Yi Zheng, Nilesh Jain
    <br>_LREC-COLING 2024_ · [Paper](https://aclanthology.org/2024.lrec-main.940/) · [Code](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning)

*   **SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL**
    <br>Ruichu Cai, **Jinjie Yuan** (Advisor 1st, Student 2nd), et al.
    <br>_NeurIPS 2021_ · [Paper](https://arxiv.org/abs/2111.00653) · [Code](https://github.com/DMIRLAB-Group/SADGA)


Patents 📝
======
*   **Methods and apparatus for enabling efficient fine-tuning on unstructured sparse and low-precision large pre-trained foundation models**
    <br>J. Pablo Muñoz, **Jinjie Yuan**, Nilesh Jain
    <br>_US Patent App. 18/935,223_ (2025)
